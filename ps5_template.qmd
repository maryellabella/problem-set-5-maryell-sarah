---
title: "PS5: Web Scraping"
author: "Maryell Abella & Sarah Kim"
date: "11/9/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Maryell Abella, maryell
    - Partner 2 (name and cnet ID): Sarah Kim, sarahk1
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*MA\*\* \*\*SK\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*0\*\* Late coins left after submission: \*\*4\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup
import warnings 
import re
from time import sleep
from datetime import datetime
import geopandas as gpd
import matplotlib.pyplot as plt
from requests.exceptions import RequestException
import concurrent.futures
import matplotlib.colors as colors
import os
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
```

```{python}
enforcement_actions = []
main_tag = soup.find('main')
li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))
li_with_div_content = [li.contents for li in li_with_div]

for li in li_with_div_content[:19]:  
    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')
    
    title_tag = li_soup.select_one('h2 a')
    title = title_tag.get_text(strip=True) 
    link = title_tag['href'] 
    
    date_tag = li_soup.select_one('span')
    date = date_tag.get_text(strip=True) 
    
    category_tag = li_soup.select_one('ul li')
    category = category_tag.get_text(strip=True)
    
    enforcement_actions.append({
        'Title': title,
        'Link': f"https://oig.hhs.gov{link}",
        'Date': date,
        'Category': category
    })

enforcement_df = pd.DataFrame(enforcement_actions)
print(enforcement_df.head())
```

### 2. Crawling (PARTNER 1)
2. (Partner 1) Crawling: Then for each enforcement action, click the link and collect
the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office,
Eastern District of Washington).
Update your dataframe with the name of the agency and print its head.


```{python}
url_1 = "https://oig.hhs.gov/fraud/enforcement/north-texas-medical-center-pays-142-million-to-resolve-potential-false-claims-act-liability-for-self-reported-violations-of-medicare-regs-stark-law/"

response_1 = requests.get(url_1)
soup_1= BeautifulSoup(response_1.content, 'html.parser')

li_tags = soup_1.find_all('li')
for li in li_tags:
    agency_span = li.find('span')
    
    if agency_span and "Agency" in agency_span.text:  
        agency_name = agency_span.find_next_sibling(text=True)
        
        if agency_name:
            print(agency_name.strip())
```


```{python}
agency_names = []

for link in enforcement_df['Link']:
    response_1 = requests.get(link)
    soup_1 = BeautifulSoup(response_1.content, 'html.parser')
    
    li_tags = soup_1.find_all('li')
    agency_name = None
    
    for li in li_tags:
        agency_span = li.find('span')
        
        if agency_span and "Agency" in agency_span.text: 
            agency_name = agency_span.find_next_sibling(text=True)
            if agency_name:
                agency_name = agency_name.strip()  

    agency_names.append(agency_name)

enforcement_df['Agency'] = agency_names
print(enforcement_df.head())
```



## Step 2: Making the scraper dynamic
Turning the scraper into a function: You will write a function that takes as input
a month and a year, and then pulls and formats the enforcement actions like in Step 1
starting from that month+year to today.

• This function should first check that the year inputted >= 2013 before starting to
scrape. If the year inputted < 2013, it should print a statement reminding the user
to restrict to year >= 2013, since only enforcement actions after 2013 are listed.
• It should save the dataframe output into a .csv file named as “enforcement_actions_
year_month.csv” (do not commit this file to git)
• If you’re crawling multiple pages, always add 1 second wait before going to the next
page to prevent potential server-side block. To implement this in Python, you may
look up .sleep() function from time library.

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2) (Partner 2) Before writing out your function, write down pseudo-code of the steps that
your function will go through. If you use a loop, discuss what kind of loop you will use
and how you will define it.

1. Define the Function
I would create a function called scrape_enforcement_actions(month, year) that takes month and year as inputs.
2. Input Validation
Check if the year is greater than or equal to 2013:
If year < 2013, print a message reminding the user that only actions after 2013 are available and return None.
If the year is valid, proceed to the next steps.
3. Initialize Variables
Create an empty list enforcement_actions to store dictionaries containing enforcement action data.
4. Loop through Pages
Use a while loop to iterate over the pages of the enforcement actions website.
Set a variable page_url to the URL of the first page (e.g., "https://oig.hhs.gov/fraud/enforcement/") or the dynamically generated URL for the current page.

Inside the Loop:
  1. Scrape Page Content
    Fetch the page using requests and parse it with BeautifulSoup.
    Extract each enforcement action’s title, date, category, and link.
  2. Filter by Date
    For each action, check if its date is later than or equal to the input month and year. If the date is older, break out of the loop to avoid scraping unnecessary pages.
  3. Store Data
    For each enforcement action that meets the date criteria, add its details to enforcement_actions.
  4. Navigate to Next Page
    Check if there’s a "Next" button or link to continue scraping additional pages.
    If a "Next" page exists, update page_url to the URL of the next page and wait 1 second using time.sleep(1) to prevent server overload.
    If no "Next" page is found, exit the loop.
5. Crawl Additional Details
For each action in enforcement_actions, access its link to scrape the name of the agency involved.
Append the agency name to the dictionary associated with each enforcement action.
6. Create and Save DataFrame
Convert enforcement_actions into a Pandas DataFrame.
Save the DataFrame as a .csv file named enforcement_actions_year_month.csv based on the input year and month.
7. Return or Print Results
Print the first few rows of the DataFrame or return it as an output of the function for verification.


* b. Create Dynamic Scraper (PARTNER 2) Now code up your dynamic scraper and run it to start collecting the enforcement
actions since January 2023. How many enforcement actions do you get in your final
dataframe? What is the date and details of the earliest enforcement action it scraped?

```{python}
def scrape_enforcement_actions(month, year):
    """Scrape enforcement actions and associated agency information."""

    if year < 2013:
        print("Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.")
        return None

    base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
    start_date = datetime(year, month, 1)
    all_actions = []
    max_workers = 10
    page_num = 1
    extra_pages_after_stop = 15 
    stop_scraping = False
    pages_scraped_after_stop = 0

    def scrape_page(page_num):
        page_url = base_url + str(page_num)
        actions = []
        try:
            response = requests.get(page_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            main_tag = soup.find('main')
            li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))

            if not li_with_div:
                return actions, True  

            for li in li_with_div:
                li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')

                title_tag = li_soup.select_one('h2 a')
                date_tag = li_soup.select_one('span')
                category_tag = li_soup.select_one('ul li')

                title = title_tag.get_text(strip=True) if title_tag else None
                link = title_tag['href'] if title_tag else None
                date_str = date_tag.get_text(strip=True) if date_tag else None
                category = category_tag.get_text(strip=True) if category_tag else None

                try:
                    date = datetime.strptime(date_str, "%B %d, %Y")
                except ValueError:
                    continue

                if date < start_date:
                    return actions, "STOP"  

                agency_name = scrape_agency_name(f"https://oig.hhs.gov{link}") if link else "NA"

                actions.append({
                    'Title': title,
                    'Link': f"https://oig.hhs.gov{link}" if link else None,
                    'Date': date,
                    'Category': category,
                    'Agency': agency_name
                })

            return actions, "CONTINUE"
        except Exception as e:
            print(f"Error scraping page {page_num}: {e}")
            return [], "CONTINUE"

    """Function to scrape agency name from the enforcement action detail page"""
    def scrape_agency_name(link):
        try:
            response = requests.get(link, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            li_tags = soup.find_all('li')
            for li in li_tags:
                agency_span = li.find('span')
                if agency_span and "Agency" in agency_span.text:
                    agency_name = agency_span.find_next_sibling(text=True)
                    if agency_name:
                        return agency_name.strip()
            return "NA"
        except requests.exceptions.RequestException as e:
            print(f"Retrying for agency details: {e}")
            return "NA"

    while not stop_scraping:
        pages_to_scrape = list(range(page_num, page_num + max_workers))
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_page = {executor.submit(scrape_page, p): p for p in pages_to_scrape}
            for future in concurrent.futures.as_completed(future_to_page):
                page = future_to_page[future]
                try:
                    actions, stop_flag = future.result()
                    all_actions.extend(actions)

                    if stop_flag == "STOP":
                        pages_scraped_after_stop += 1
                        if pages_scraped_after_stop >= extra_pages_after_stop:
                            stop_scraping = True
                            break
                except Exception as e:
                    print(f"Error processing page {page}: {e}")
        page_num += max_workers

    """Create a DataFrame from collected enforcement actions"""
    enforcement_df = pd.DataFrame(all_actions)

    return enforcement_df


"""Run the function to collect data from January 2023 onward"""
enforcement_df = scrape_enforcement_actions(1, 2023)
print("Number of enforcement actions collected:", len(enforcement_df))
if not enforcement_df.empty:
    print("Earliest enforcement action:", enforcement_df.sort_values(by='Date').iloc[0])

#used chatgpt to debug because it was taking ages to run the function
#https://docs.python.org/3/library/concurrent.futures.html
#https://www.geeksforgeeks.org/how-to-use-threadpoolexecutor-in-python3/
#used chatgpt to figure out concurrent.futures

```

```{python}
print("Number of enforcement actions collected:", len(enforcement_df))
if not enforcement_df.empty:
    print("Earliest enforcement action:", enforcement_df.sort_values(by='Date').iloc[0])
```

* c. Test Partner's Code (PARTNER 1) Now, let’s go a little further back. Test your partner’s code by collecting
the actions since January 2021. Note that this can take a while. How many enforcement
actions do you get in your final dataframe? What is the date and details of the earliest
enforcement action it scraped? Use the dataframe from this process for every question
after this.


```{python}
def scrape_enforcement_actions(month, year):
    """Scrape enforcement actions and associated agency information."""

    if year < 2013:
        print("Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.")
        return None

    base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
    start_date = datetime(year, month, 1)
    all_actions = []
    max_workers = 10
    page_num = 1
    extra_pages_after_stop = 15 
    stop_scraping = False
    pages_scraped_after_stop = 0

    def scrape_page(page_num):
        page_url = base_url + str(page_num)
        actions = []
        try:
            response = requests.get(page_url, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            main_tag = soup.find('main')
            li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))

            if not li_with_div:
                return actions, True  

            for li in li_with_div:
                li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')

                title_tag = li_soup.select_one('h2 a')
                date_tag = li_soup.select_one('span')
                category_tag = li_soup.select_one('ul li')

                title = title_tag.get_text(strip=True) if title_tag else None
                link = title_tag['href'] if title_tag else None
                date_str = date_tag.get_text(strip=True) if date_tag else None
                category = category_tag.get_text(strip=True) if category_tag else None

                try:
                    date = datetime.strptime(date_str, "%B %d, %Y")
                except ValueError:
                    continue

                if date < start_date:
                    return actions, "STOP"  

                agency_name = scrape_agency_name(f"https://oig.hhs.gov{link}") if link else "NA"

                actions.append({
                    'Title': title,
                    'Link': f"https://oig.hhs.gov{link}" if link else None,
                    'Date': date,
                    'Category': category,
                    'Agency': agency_name
                })

            return actions, "CONTINUE"
        except Exception as e:
            print(f"Error scraping page {page_num}: {e}")
            return [], "CONTINUE"

    """Function to scrape agency name from the enforcement action detail page"""
    def scrape_agency_name(link):
        try:
            response = requests.get(link, timeout=10)
            response.raise_for_status()
            soup = BeautifulSoup(response.content, 'html.parser')

            li_tags = soup.find_all('li')
            for li in li_tags:
                agency_span = li.find('span')
                if agency_span and "Agency" in agency_span.text:
                    agency_name = agency_span.find_next_sibling(text=True)
                    if agency_name:
                        return agency_name.strip()
            return "NA"
        except requests.exceptions.RequestException as e:
            print(f"Retrying for agency details: {e}")
            return "NA"

    while not stop_scraping:
        pages_to_scrape = list(range(page_num, page_num + max_workers))
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            future_to_page = {executor.submit(scrape_page, p): p for p in pages_to_scrape}
            for future in concurrent.futures.as_completed(future_to_page):
                page = future_to_page[future]
                try:
                    actions, stop_flag = future.result()
                    all_actions.extend(actions)

                    if stop_flag == "STOP":
                        pages_scraped_after_stop += 1
                        if pages_scraped_after_stop >= extra_pages_after_stop:
                            stop_scraping = True
                            break
                except Exception as e:
                    print(f"Error processing page {page}: {e}")
        page_num += max_workers

    """Create a DataFrame from collected enforcement actions"""
    enforcement_df = pd.DataFrame(all_actions)

    return enforcement_df

"""Run the function to collect data from January 2021 onward"""
enforcement_df = scrape_enforcement_actions(1, 2021)
```

```{python}
print("Number of enforcement actions collected:", len(enforcement_df))
if not enforcement_df.empty:
    print("Earliest enforcement action:", enforcement_df.sort_values(by='Date').iloc[0])
```


## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)
1. (Partner 2) Plot a line chart that shows: the number of enforcement actions over
time (aggregated to each month+year) overall since January 2021,

```{python}
enforcement_df['Date'] = pd.to_datetime(enforcement_df['Date'])

enforcement_df['YearMonth'] = enforcement_df['Date'].dt.to_period('M')
enforcement_counts = enforcement_df.groupby('YearMonth').size().reset_index(name='Count')

enforcement_counts['YearMonth'] = enforcement_counts['YearMonth'].dt.to_timestamp()

"""Plotting with Altair"""
chart = alt.Chart(enforcement_counts).mark_line().encode(
    x=alt.X('YearMonth:T', title='Year and Month', axis=alt.Axis(format='%Y-%m')), 
    y=alt.Y('Count:Q', title='Count of Enforcement Actions')
).properties(
    title='Number of Enforcement Actions Over Time'
)

chart.show()

#source: https://docs.python.org/3/library/datetime.html
#https://stackoverflow.com/questions/70875690/change-date-axis-ticks-in-altair-to-show-years
```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)
2. (Partner 1) Plot a line chart that shows: the number of enforcement actions split
out by:
• “Criminal and Civil Actions” vs. “State Enforcement Agencies”

```{python}
enforcement_categories = ["Criminal and Civil Actions", "State Enforcement Agencies"]
filtered_df = enforcement_df[enforcement_df['Category'].isin(enforcement_categories)]

filtered_df['Date'] = pd.to_datetime(filtered_df['Date'])
actions_by_date = filtered_df.groupby([filtered_df['Date'].dt.to_period('M'), 'Category']).size().reset_index(name='Count')
actions_by_date['Date'] = actions_by_date['Date'].dt.to_timestamp()

chart = alt.Chart(actions_by_date).mark_line().encode(
    x=alt.X('Date:T', axis=alt.Axis(format='%b %Y')), 
    y='Count:Q',
    color='Category:N',  
).properties(
    title='Enforcement Actions by Category'
)

chart.show()
```

• Five topics in the “Criminal and Civil Actions” category: “Health Care Fraud”,
“Financial Fraud”, “Drug Enforcement”, “Bribery/Corruption”, and “Other”. Hint:
You will need to divide the five topics manually by looking at the title and assigning
the relevant topic. For example, if you find the word “bank” or “financial” in the
title of an action, then that action should probably belong to “Financial Fraud”
topic.

```{python}
cc_actions = enforcement_df[enforcement_df['Category'] == 'Criminal and Civil Actions']

keywords = {
    "Health Care Fraud": ["health", "healthcare", "medicaid", "medicare", "medical", "medication", "hospital", "pharmacy", "pharmaceutical", "pharmacist",  "doctor", "insurance", "covid", "physician", "prescription", "kickback", "surgeon", "nurse", "nursing", "clinic"],
    "Financial Fraud": ["bank", "financial", "finance", "capital", "fraudulant", "investment", "loan", "credit", "tax evasion", "embezzlement", "laundering", "money laundering", "tax fraud", "insider trading", "grants", "assets", "identity theft", "consumer", "credit card", "chargeback"],
    "Drug Enforcement": ["drug", "addiction", "opioid", "narcotic", "fentanyl", "cocaine", "substance", "marijuana", "DEA"],
    "Bribery/Corruption": ["bribery", "bribe", "corruption", "corrupt", "payoff", "extort", "payment"],
    "Other": [] 
}

def assign_topic(title):
    title = title.lower() 
    for topic, words in keywords.items():
        if any(word in title for word in words):  
            return topic
    return "Other"  

cc_actions['Topic'] = cc_actions['Title'].apply(assign_topic)

print(cc_actions[['Title', 'Topic']].head(10))
```


```{python}
cc_actions['Date'] = pd.to_datetime(cc_actions['Date'])
cc_topics = ["Health Care Fraud", "Financial Fraud", "Drug Enforcement", "Bribery/Corruption", "Other"]
filtered_cc_actions = cc_actions[cc_actions['Topic'].isin(cc_topics)]

actions_by_date = filtered_cc_actions.groupby([cc_actions['Date'].dt.to_period('M'), 'Topic']).size().reset_index(name='Count')
actions_by_date['Date'] = actions_by_date['Date'].dt.to_timestamp()


chart = alt.Chart(actions_by_date).mark_line().encode(
    x=alt.X('Date:T', axis=alt.Axis(format='%b %Y')), 
    y='Count:Q', 
    color='Topic:N',  
).properties(
    title='Enforcement Actions by Topic'
)

chart.show()
```

## Step 4: Create maps of enforcement activity
For these questions, use this US Attorney District shapefile (link) and a Census state shapefile
(link)

### 1. Map by State (PARTNER 1)
(Partner 1) Map by state: Among actions taken by state-level agencies, clean the state
names you collected and plot a choropleth of the number of enforcement actions for each
state. Hint: look for “State of” in the agency info!

```{python}
path = r'/Users/maryell/Desktop/problem-set-5-maryell-sarah/'

census_state_folder = 'cb_2018_us_state_500k'
census_state_path = os.path.join(path, census_state_folder, 'cb_2018_us_state_500k.shp')
states_shapefile = gpd.read_file(census_state_path)

states_shapefile.head()
```

```{python}
state_name_to_abbr = {
    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR',
    'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE',
    'Florida': 'FL', 'Georgia': 'GA', 'Hawaii': 'HI', 'Idaho': 'ID',
    'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',
    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD',
    'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS',
    'Missouri': 'MO', 'Montana': 'MT', 'Nebraska': 'NE', 'Nevada': 'NV',
    'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY',
    'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',
    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',
    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',
    'Vermont': 'VT', 'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV',
    'Wisconsin': 'WI', 'Wyoming': 'WY'
}

"""Extract state abbreviation from a state name found in the agency information."""
def extract_state_abbr(agency_name):
    if 'district' in agency_name.lower():
        return None  
    
    for state_name in state_name_to_abbr:
        pattern = r'\b' + re.escape(state_name) + r'\b'
        match = re.search(pattern, agency_name, re.IGNORECASE)        
        if match:
            return state_name_to_abbr[state_name]
    return None

enforcement_df['State_abbr'] = enforcement_df['Agency'].apply(extract_state_abbr)

state_counts = enforcement_df['State_abbr'].value_counts().reset_index()
state_counts.columns = ['State_abbr', 'Action_Count']

states_shapefile = states_shapefile.rename(columns={'STUSPS': 'State_abbr'})
merged = states_shapefile.merge(state_counts, on='State_abbr', how='left')
```

```{python}
fig, ax = plt.subplots()
merged = merged.to_crs(epsg=5070)
merged.plot(column='Action_Count', ax=ax, legend=True,
            legend_kwds={'label': "Number of Enforcement Actions by State", 'orientation': 'horizontal'},
            cmap='Blues')

ax.set_title('Enforcement Actions by State')
ax.set_xticks([])
ax.set_yticks([])

plt.show()
```

### 2. Map by District (PARTNER 2)
(Partner 2) Map by district: Among actions taken by US Attorney District-level agencies,
clean the district names so that you can merge them with the shapefile, and then
plot a choropleth of the number of enforcement actions in each US Attorney District.
Hint: look for “District” in the agency info.

```{python}
districts_folder = 'US_Attorney_Districts_Shapefile_simplified_20241107'
districts_path = os.path.join(path, district_attorney_folder, 
'geo_export_a70ac548-6b1a-49a8-a51c-4c7108365975.shp')
districts_gdf = gpd.read_file(district_attorney_path)
```

```{python}
"""Filter actions taken by US Attorney District-level agencies"""
district_actions = enforcement_df[enforcement_df['Agency'].str.contains('District', case=False, na=False)].copy()

"""Extract and clean district names"""
def extract_district_name(agency):
    if pd.isnull(agency):
        return None
    match = re.search(r'([A-Za-z\s]+District\s+of\s+[A-Za-z\s]+|District\s+of\s+[A-Za-z\s]+)', agency)
    if match:
        district_name = match.group(0).strip()
        return district_name
    else:
        return None

district_actions['District'] = district_actions['Agency'].apply(extract_district_name)
district_actions = district_actions.dropna(subset=['District'])

"""Standardize district names"""
def standardize_district_name(name):
    if pd.isnull(name):
        return None
    name = name.lower().strip()
    name = re.sub(r'(u\.s\.\s+)?district\s+of\s+', '', name)
    name = name.replace('district', '').strip()
    name = re.sub(r'\s+', '_', name)
    name = re.sub(r'[^\w_]', '', name)
    return name

district_actions['District_Clean'] = district_actions['District'].apply(standardize_district_name)

"""Identify the correct column in the shapefile"""
district_column_name = 'judicial_d'  
districts_gdf['District_Clean'] = districts_gdf[district_column_name].apply(standardize_district_name)

"""Aggregate counts and merge"""
district_counts = district_actions.groupby('District_Clean').size().reset_index(name='Count')
merged_gdf = districts_gdf.merge(district_counts, on='District_Clean', how='left')
merged_gdf['Count'] = merged_gdf['Count'].fillna(0).astype(int)

merged_gdf = merged_gdf.to_crs("ESRI:102003")
```

```{python}
"""Plot the choropleth map"""
fig, ax = plt.subplots(1, 1, figsize=(20, 12))

max_count = merged_gdf['Count'].max()
norm = colors.Normalize(vmin=0, vmax=max_count)

"""Plot the merged GeoDataFrame"""
merged_gdf.plot(
    column='Count',
    ax=ax,
    legend=True,
    cmap='OrRd',
    edgecolor='black',
    norm=norm,
    missing_kwds={'color': 'lightgrey', 'hatch': '///'},
)

ax.set_title('Number of Enforcement Actions by US Attorney District', fontsize=15)

ax.axis('off')

plt.show()

#source: asked chatgpt to debug missing values from states since the whole western region was zero for me for a while
#https://stackoverflow.com/questions/20240239/python-re-search

```

## Extra Credit

### 1. Merge zip code shapefile with population
Use the zip code shapefile from the previous problem set and merge it with zip codelevel
population data. (Go to Census Data Portal, select “ZIP Code Tabulation Area”,
check “All 5-digit ZIP Code Tabulation Areas within United States”, and under “P1
TOTAL POPULATION” select “2020: DEC Demographic and Housing Characteristics”.
Download the csv.).

```{python}
census_zip_folder = 'gz_2010_us_860_00_500k'
census_zip_path = os.path.join(path, census_zip_folder, 'gz_2010_us_860_00_500k.shp')
zip_shapefile = gpd.read_file(census_zip_path)
```

```{python}
population_data_folder = 'DECENNIALDHC2020.P1_2024-11-08T190918'
population_data_path = os.path.join(path, population_data_folder, 'DECENNIALDHC2020.P1-Data.csv')
population_data = pd.read_csv(population_data_path)
print("Columns in population data:", population_data.columns)
print(population_data.head())
```

```{python}
"""Format ZIP codes in population data and rename population column"""
population_data = population_data.drop(index=0)
population_data['ZIP'] = population_data['NAME'].str.replace('ZCTA5 ', '').str.zfill(5)
population_data = population_data[['ZIP', 'P1_001N']]
population_data = population_data.rename(columns={'P1_001N': 'Population'})

zip_shapefile = zip_shapefile.rename(columns={'ZCTA5': 'ZIP'})

"""Merge shapefile and population data"""
merged_zip_data = zip_shapefile.merge(population_data[['ZIP', 'Population']], on='ZIP', how='left')
merged_zip_data['Population'] = merged_zip_data['Population'].fillna(0)

"""Display results and summary"""
print("Unmatched ZIPs after merge (first 10):")
print(merged_zip_data[merged_zip_data['Population'] == 0][['ZIP']].head(10))
print("\nSample of merged_zip_data (first 5 rows):")
print(merged_zip_data.head())
print("\nSummary of 'Population' in merged data after handling missing values:")
print(merged_zip_data['Population'].describe())
```

### 2. Conduct spatial join
Conduct a spatial join between zip code shapefile and the district shapefile, then aggregate
to get population in each district.

```{python}
merged_with_districts = gpd.sjoin(merged_zip_data, districts_gdf, how="left", predicate="intersects")

# Print columns after spatial join to check if 'Population' is still there
print("Columns in merged_with_districts after spatial join:", merged_with_districts.columns)
```


```{python}
print("CRS of zip_shapefile:", zip_shapefile.crs)
print("CRS of districts_gdf:", districts_gdf.crs)

```


```{python}
zip_shapefile = zip_shapefile.to_crs(districts_gdf.crs)

Columns in merged_zip_data before spatial join: Index(['GEO_ID', 'ZIP', 'NAME', 'LSAD', # Perform the spatial join between zip_shapefile and districts_gdf
merged_with_districts = gpd.sjoin(merged_zip_data, districts_gdf, how="left", predicate="intersects")

# Check columns in merged_with_districts after the spatial join
print("Columns in merged_with_districts after spatial join:", merged_with_districts.columns)

# Check if 'Population' column exists after the join
if 'Population' in merged_with_districts.columns:
    print("Population column is present.")
else:
    print("Population column is missing.")

```


```{python}
# Step 2: Perform the spatial join
# This joins the zip code polygons with the districts by spatial relationship (intersects)
zip_shapefile = zip_shapefile.to_crs(districts_gdf.crs)

# Perform the spatial join between ZIP code shapefile and district shapefile
merged_with_districts = gpd.sjoin(zip_shapefile, districts_gdf, how="left", predicate="intersects")

# Ensure the 'Population' column is numeric and fill missing values with 0
merged_with_districts['Population'] = pd.to_numeric(merged_with_districts['Population'], errors='coerce')
merged_with_districts['Population'] = merged_with_districts['Population'].fillna(0)

# Aggregate population by district (group by 'district_n')
population_by_district = merged_with_districts.groupby('district_n')['Population'].sum().reset_index()

# Display the result
print(population_by_district.head())

```


### 3. Map the action ratio in each district
Map the ratio of enforcement actions in each US Attorney District. You can calculate the
ratio by aggregating the number of enforcement actions since January 2021 per district,
and dividing it with the population data.
```{python}

```