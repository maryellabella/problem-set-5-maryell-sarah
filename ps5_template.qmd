---
title: "PS5: Web Scraping"
author: "Maryell Abella & Sarah Kim"
date: "11/9/2024"
format: 
  pdf:
    include-in-header: 
       text: |
         \usepackage{fvextra}
         \DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\{\}}
include-before-body:
  text: |
    \RecustomVerbatimEnvironment{verbatim}{Verbatim}{
      showspaces = false,
      showtabs = false,
      breaksymbolleft={},
      breaklines
    }
output:
  echo: false
  eval: false
---

**Due 11/9 at 5:00PM Central. Worth 100 points + 10 points extra credit.**

## Submission Steps (10 pts)
1. This problem set is a paired problem set.
2. Play paper, scissors, rock to determine who goes first. Call that person *Partner 1*.
    - Partner 1 (name and cnet ID): Maryell Abella, maryell
    - Partner 2 (name and cnet ID): Sarah Kim, sarahk1
3. Partner 1 will accept the `ps5` and then share the link it creates with their partner. You can only share it with one partner so you will not be able to change it after your partner has accepted. 
4. "This submission is our work alone and complies with the 30538 integrity policy." Add your initials to indicate your agreement: \*\*MA\*\* \*\*SK\*\*
5. "I have uploaded the names of anyone else other than my partner and I worked with on the problem set **[here](https://docs.google.com/forms/d/185usrCREQaUbvAXpWhChkjghdGgmAZXA3lPWpXLLsts/edit)**"  (1 point)
6. Late coins used this pset: \*\*0\*\* Late coins left after submission: \*\*4\*\*
7. Knit your `ps5.qmd` to an PDF file to make `ps5.pdf`, 
    * The PDF should not be more than 25 pages. Use `head()` and re-size figures when appropriate. 
8. (Partner 1): push  `ps5.qmd` and `ps5.pdf` to your github repo.
9. (Partner 1): submit `ps5.pdf` via Gradescope. Add your partner on Gradescope.
10. (Partner 1): tag your submission in Gradescope

\newpage

```{python}
import pandas as pd
import altair as alt
import time
import requests
from bs4 import BeautifulSoup
import warnings 
import re
from time import sleep
from datetime import datetime
warnings.filterwarnings('ignore')
alt.renderers.enable("png")
```


## Step 1: Develop initial scraper and crawler

### 1. Scraping (PARTNER 1)

1. (Partner 1) Scraping: Go to the first page of the HHS OIG’s “Enforcement Actions”
page and scrape and collect the following into a dataset:
• Title of the enforcement action
• Date
• Category (e.g, “Criminal and Civil Actions”)
• Link associated with the enforcement action

```{python}
url = "https://oig.hhs.gov/fraud/enforcement/"
response = requests.get(url)
soup = BeautifulSoup(response.text, 'lxml')
```

```{python}
enforcement_actions = []
main_tag = soup.find('main')
li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))
li_with_div_content = [li.contents for li in li_with_div]

for li in li_with_div_content[:19]:  
    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')
    
    title_tag = li_soup.select_one('h2 a')
    title = title_tag.get_text(strip=True) 
    link = title_tag['href'] 
    
    date_tag = li_soup.select_one('span')
    date = date_tag.get_text(strip=True) 
    
    category_tag = li_soup.select_one('ul li')
    category = category_tag.get_text(strip=True)
    
    enforcement_actions.append({
        'Title': title,
        'Link': f"https://oig.hhs.gov{link}",
        'Date': date,
        'Category': category
    })

enforcement_df = pd.DataFrame(enforcement_actions)
print(enforcement_df.head())
```

### 2. Crawling (PARTNER 1)
2. (Partner 1) Crawling: Then for each enforcement action, click the link and collect
the name of the agency involved (e.g., for this link, it would be U.S. Attorney’s Office,
Eastern District of Washington).
Update your dataframe with the name of the agency and print its head.


```{python}
url_1 = "https://oig.hhs.gov/fraud/enforcement/north-texas-medical-center-pays-142-million-to-resolve-potential-false-claims-act-liability-for-self-reported-violations-of-medicare-regs-stark-law/"

response_1 = requests.get(url_1)
soup_1= BeautifulSoup(response_1.content, 'html.parser')

li_tags = soup_1.find_all('li')
for li in li_tags:
    agency_span = li.find('span')
    
    if agency_span and "Agency" in agency_span.text:  
        agency_name = agency_span.find_next_sibling(text=True)
        
        if agency_name:
            print(agency_name.strip())
```


```{python}
agency_names = []

for link in enforcement_df['Link']:
    response_1 = requests.get(link)
    soup_1 = BeautifulSoup(response_1.content, 'html.parser')
    
    li_tags = soup_1.find_all('li')
    agency_name = None
    
    for li in li_tags:
        agency_span = li.find('span')
        
        if agency_span and "Agency" in agency_span.text: 
            agency_name = agency_span.find_next_sibling(text=True)
            if agency_name:
                agency_name = agency_name.strip()  

    agency_names.append(agency_name)

enforcement_df['Agency'] = agency_names
print(enforcement_df.head())
```


## Step 2: Making the scraper dynamic
Turning the scraper into a function: You will write a function that takes as input
a month and a year, and then pulls and formats the enforcement actions like in Step 1
starting from that month+year to today.

• This function should first check that the year inputted >= 2013 before starting to
scrape. If the year inputted < 2013, it should print a statement reminding the user
to restrict to year >= 2013, since only enforcement actions after 2013 are listed.
• It should save the dataframe output into a .csv file named as “enforcement_actions_
year_month.csv” (do not commit this file to git)
• If you’re crawling multiple pages, always add 1 second wait before going to the next
page to prevent potential server-side block. To implement this in Python, you may
look up .sleep() function from time library.

### 1. Turning the scraper into a function 

* a. Pseudo-Code (PARTNER 2) (Partner 2) Before writing out your function, write down pseudo-code of the steps that
your function will go through. If you use a loop, discuss what kind of loop you will use
and how you will define it.

1. Define the Function
I would create a function called scrape_enforcement_actions(month, year) that takes month and year as inputs.
2. Input Validation
Check if the year is greater than or equal to 2013:
If year < 2013, print a message reminding the user that only actions after 2013 are available and return None.
If the year is valid, proceed to the next steps.
3. Initialize Variables
Create an empty list enforcement_actions to store dictionaries containing enforcement action data.
4. Loop through Pages
Use a while loop to iterate over the pages of the enforcement actions website.
Set a variable page_url to the URL of the first page (e.g., "https://oig.hhs.gov/fraud/enforcement/") or the dynamically generated URL for the current page.
Inside the Loop:
  1. Scrape Page Content
    Fetch the page using requests and parse it with BeautifulSoup.
    Extract each enforcement action’s title, date, category, and link.
  2. Filter by Date
    For each action, check if its date is later than or equal to the input month and year. If the date is older, break out of the loop to avoid scraping unnecessary pages.
  3. Store Data
    For each enforcement action that meets the date criteria, add its details to enforcement_actions.
  4. Navigate to Next Page
    Check if there’s a "Next" button or link to continue scraping additional pages.
    If a "Next" page exists, update page_url to the URL of the next page and wait 1 second using time.sleep(1) to prevent server overload.
    If no "Next" page is found, exit the loop.
5. Crawl Additional Details
For each action in enforcement_actions, access its link to scrape the name of the agency involved.
Append the agency name to the dictionary associated with each enforcement action.
6. Create and Save DataFrame
Convert enforcement_actions into a Pandas DataFrame.
Save the DataFrame as a .csv file named enforcement_actions_year_month.csv based on the input year and month.
7. Return or Print Results
Print the first few rows of the DataFrame or return it as an output of the function for verification.


* b. Create Dynamic Scraper (PARTNER 2) Now code up your dynamic scraper and run it to start collecting the enforcement
actions since January 2023. How many enforcement actions do you get in your final
dataframe? What is the date and details of the earliest enforcement action it scraped?

```{python}
def scrape_enforcement_actions(month, year):
    if year < 2013:
        print("Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.")
        return None
    
    """Initialize list to store enforcement actions"""
    enforcement_actions = []
    base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
    page_num = 1  # Start from the first page
    start_date = datetime(year, month, 1)

    while True:
        page_url = base_url + str(page_num)
        response = requests.get(page_url)
        
        if response.status_code != 200:
            print(f"Failed to retrieve data from page {page_num}")
            break
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        main_tag = soup.find('main')
        li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))
        
        for li in li_with_div:
            li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')
            
            title_tag = li_soup.select_one('h2 a')
            date_tag = li_soup.select_one('span')
            category_tag = li_soup.select_one('ul li')
            
            title = title_tag.get_text(strip=True) if title_tag else None
            link = title_tag['href'] if title_tag else None
            date_str = date_tag.get_text(strip=True) if date_tag else None
            category = category_tag.get_text(strip=True) if category_tag else None
            
            try:
                date = datetime.strptime(date_str, "%B %d, %Y")
            except ValueError:
                continue  
            
            if date < start_date:
                return pd.DataFrame(enforcement_actions)
            
            enforcement_actions.append({
                'Title': title,
                'Link': f"https://oig.hhs.gov{link}",
                'Date': date,
                'Category': category
            })
        
        page_num += 1
        sleep(1)  

    """Final DataFrame after collecting all enforcement actions"""
    enforcement_df = pd.DataFrame(enforcement_actions)
    return enforcement_df

"""Run the function to collect data from January 2023 onward"""
enforcement_df = scrape_enforcement_actions(1, 2023)

```

```{python}
total_actions = len(enforcement_df)
print("Total enforcement actions since January 2023:", total_actions)

earliest_action = enforcement_df.sort_values(by='Date').iloc[0]
print("Earliest enforcement action:\n", earliest_action)
```


* c. Test Partner's Code (PARTNER 1) Now, let’s go a little further back. Test your partner’s code by collecting
the actions since January 2021. Note that this can take a while. How many enforcement
actions do you get in your final dataframe? What is the date and details of the earliest
enforcement action it scraped? Use the dataframe from this process for every question
after this.

```{python}
def scrape_enforcement_actions(month, year):
    if year < 2013:
        print("Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.")
        return None
    
    """Initialize an empty list to store enforcement actions"""
    enforcement_actions = []
    base_url = "https://oig.hhs.gov/fraud/enforcement/?page="
    page_num = 1  
    start_date = datetime(year, month, 1)

    while True:
        page_url = base_url + str(page_num)
        response = requests.get(page_url)
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        main_tag = soup.find('main')
        li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))
        
        for li in li_with_div:
            li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')
            
            title_tag = li_soup.select_one('h2 a')
            date_tag = li_soup.select_one('span')
            category_tag = li_soup.select_one('ul li')
            
            title = title_tag.get_text(strip=True) if title_tag else None
            link = title_tag['href'] if title_tag else None
            date_str = date_tag.get_text(strip=True) if date_tag else None
            category = category_tag.get_text(strip=True) if category_tag else None
            
            try:
                date = datetime.strptime(date_str, "%B %d, %Y")
            except ValueError:
                continue 
            
            if date < start_date:
                return pd.DataFrame(enforcement_actions)

            enforcement_actions.append({
                'Title': title,
                'Link': f"https://oig.hhs.gov{link}",
                'Date': date,
                'Category': category
            })
        
        page_num += 1
        sleep(1)  

    enforcement_df = pd.DataFrame(enforcement_actions)
    return enforcement_df

"""Run the function to collect data from January 2021 onward"""
enforcement_df = scrape_enforcement_actions(1, 2021)
```


```{python}
total_actions = len(enforcement_df)
print("Total enforcement actions since January 2021:", total_actions)

earliest_action = enforcement_df.sort_values(by='Date').iloc[0]
print("Earliest enforcement action:\n", earliest_action)
```

## Step 3: Plot data based on scraped data

### 1. Plot the number of enforcement actions over time (PARTNER 2)
1. (Partner 2) Plot a line chart that shows: the number of enforcement actions over
time (aggregated to each month+year) overall since January 2021,

```{python}

enforcement_df['Date'] = pd.to_datetime(enforcement_df['Date'])

enforcement_df['YearMonth'] = enforcement_df['Date'].dt.to_period('M')
enforcement_counts = enforcement_df.groupby('YearMonth').size().reset_index(name='Count')

enforcement_counts['YearMonth'] = enforcement_counts['YearMonth'].dt.to_timestamp()

"""Plotting with Altair"""
chart = alt.Chart(enforcement_counts).mark_line().encode(
    x=alt.X('YearMonth:T', title='Year and Month', axis=alt.Axis(format='%Y-%m')),  # Format to show year and month
    y=alt.Y('Count:Q', title='Count of Enforcement Actions')
).properties(
    title='Number of Enforcement Actions Over Time'
)

chart.show()


```

### 2. Plot the number of enforcement actions categorized: (PARTNER 1)
2. (Partner 1) Plot a line chart that shows: the number of enforcement actions split
out by:
• “Criminal and Civil Actions” vs. “State Enforcement Agencies”

```{python}

```

• Five topics in the “Criminal and Civil Actions” category: “Health Care Fraud”,
“Financial Fraud”, “Drug Enforcement”, “Bribery/Corruption”, and “Other”. Hint:
You will need to divide the five topics manually by looking at the title and assigning
the relevant topic. For example, if you find the word “bank” or “financial” in the
title of an action, then that action should probably belong to “Financial Fraud”
topic.

```{python}

```

## Step 4: Create maps of enforcement activity
For these questions, use this US Attorney District shapefile (link) and a Census state shapefile
(link)

### 1. Map by State (PARTNER 1)
(Partner 1) Map by state: Among actions taken by state-level agencies, clean the state
names you collected and plot a choropleth of the number of enforcement actions for each
state. Hint: look for “State of” in the agency info!
```{python}

```


### 2. Map by District (PARTNER 2)
(Partner 2) Map by district: Among actions taken by US Attorney District-level agencies,
clean the district names so that you can merge them with the shapefile, and then
plot a choropleth of the number of enforcement actions in each US Attorney District.
Hint: look for “District” in the agency info.
```{python}

```

## Extra Credit

### 1. Merge zip code shapefile with population
Use the zip code shapefile from the previous problem set and merge it with zip codelevel
population data. (Go to Census Data Portal, select “ZIP Code Tabulation Area”,
check “All 5-digit ZIP Code Tabulation Areas within United States”, and under “P1
TOTAL POPULATION” select “2020: DEC Demographic and Housing Characteristics”.
Download the csv.).

```{python}

```

### 2. Conduct spatial join
Conduct a spatial join between zip code shapefile and the district shapefile, then aggregate
to get population in each district.
```{python}

```

### 3. Map the action ratio in each district
Map the ratio of enforcement actions in each US Attorney District. You can calculate the
ratio by aggregating the number of enforcement actions since January 2021 per district,
and dividing it with the population data.
```{python}

```