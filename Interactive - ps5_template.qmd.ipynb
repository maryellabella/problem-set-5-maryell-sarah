{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Connected to Python 3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "abcd3d94-0990-4232-bca2-abb720179688",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:2\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[1;32m      3\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39mtext, \u001b[39m'\u001b[39m\u001b[39mlxml\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed005258-1e01-4dd0-b8ae-9663a7ddc3f0",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'requests'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39maltair\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39malt\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'requests'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.renderers.enable(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting requests\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "\u001b[K     |████████████████████████████████| 64 kB 2.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting charset-normalizer<4,>=2\n",
      "  Downloading charset_normalizer-3.4.0-cp39-cp39-macosx_11_0_arm64.whl (120 kB)\n",
      "\u001b[K     |████████████████████████████████| 120 kB 5.7 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /Users/sarahkim/Library/Python/3.9/lib/python/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/sarahkim/Library/Python/3.9/lib/python/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/sarahkim/Library/Python/3.9/lib/python/site-packages (from requests) (2.2.3)\n",
      "Installing collected packages: charset-normalizer, requests\n",
      "Successfully installed charset-normalizer-3.4.0 requests-2.32.3\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb3dced4-c9e6-41a2-8037-01e38b1cd2d3",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahkim/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bs4'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrequests\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mbs4\u001b[39;00m \u001b[39mimport\u001b[39;00m BeautifulSoup\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mwarnings\u001b[39;00m \n\u001b[1;32m      7\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'bs4'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.renderers.enable(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "\u001b[K     |████████████████████████████████| 147 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting soupsieve>1.2\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4, bs4\n",
      "Successfully installed beautifulsoup4-4.12.3 bs4-0.0.2 soupsieve-2.6\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ef6c24a-7699-40f6-96fa-78299bef9deb",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.renderers.enable(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c45c5de8-44a2-4caa-b497-db82724063ec",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:3\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> 3\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mtext, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement parser (from versions: none)\u001b[0m\n",
      "\u001b[31mERROR: No matching distribution found for parser\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install parser library\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8782fdd7-5a03-4eab-9c13-95de8ea71fa6",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:3\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> 3\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mtext, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting lxml\n",
      "  Downloading lxml-5.3.0-cp39-cp39-macosx_10_9_universal2.whl (8.1 MB)\n",
      "\u001b[K     |████████████████████████████████| 8.1 MB 3.0 MB/s eta 0:00:01\n",
      "\u001b[?25hInstalling collected packages: lxml\n",
      "Successfully installed lxml-5.3.0\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 24.3.1 is available.\n",
      "You should consider upgrading via the '/Library/Developer/CommandLineTools/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install lxml\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "daa72367-427b-486b-9f67-4b272233dc09",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:3\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> 3\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mtext, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8346f9b-59dc-468c-9166-7ba70060265f",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'soup' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m main_tag \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mmain\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      2\u001b[0m li_with_div \u001b[39m=\u001b[39m soup\u001b[39m.\u001b[39mfind_all(\u001b[39mlambda\u001b[39;00m t: t\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mli\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m t\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mdiv\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m li_with_div_content \u001b[39m=\u001b[39m [li\u001b[39m.\u001b[39mcontents \u001b[39mfor\u001b[39;00m li \u001b[39min\u001b[39;00m li_with_div]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'soup' is not defined"
     ]
    }
   ],
   "source": [
    "main_tag = soup.find('main')\n",
    "li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "li_with_div_content = [li.contents for li in li_with_div]\n",
    "\n",
    "for li in li_with_div_content[:19]:  \n",
    "    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')\n",
    "    \n",
    "    title_tag = li_soup.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True) \n",
    "    link = title_tag['href'] \n",
    "    \n",
    "    date_tag = li_soup.select_one('span')\n",
    "    date = date_tag.get_text(strip=True) \n",
    "    \n",
    "    category_tag = li_soup.select_one('ul li')\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    \n",
    "    enforcement_actions.append({\n",
    "        'Title': title,\n",
    "        'Link': f\"https://oig.hhs.gov{link}\",\n",
    "        'Date': date,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "68ffd859-6f47-4318-9712-99728556247d",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "FeatureNotFound",
     "evalue": "Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFeatureNotFound\u001b[0m                           Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:3\u001b[0m\n\u001b[1;32m      1\u001b[0m url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(url)\n\u001b[0;32m----> 3\u001b[0m soup \u001b[39m=\u001b[39m BeautifulSoup(response\u001b[39m.\u001b[39;49mtext, \u001b[39m'\u001b[39;49m\u001b[39mlxml\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/bs4/__init__.py:250\u001b[0m, in \u001b[0;36mBeautifulSoup.__init__\u001b[0;34m(self, markup, features, builder, parse_only, from_encoding, exclude_encodings, element_classes, **kwargs)\u001b[0m\n\u001b[1;32m    248\u001b[0m     builder_class \u001b[39m=\u001b[39m builder_registry\u001b[39m.\u001b[39mlookup(\u001b[39m*\u001b[39mfeatures)\n\u001b[1;32m    249\u001b[0m     \u001b[39mif\u001b[39;00m builder_class \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 250\u001b[0m         \u001b[39mraise\u001b[39;00m FeatureNotFound(\n\u001b[1;32m    251\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mCouldn\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt find a tree builder with the features you \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    252\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mrequested: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Do you need to install a parser library?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    253\u001b[0m             \u001b[39m%\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(features))\n\u001b[1;32m    255\u001b[0m \u001b[39m# At this point either we have a TreeBuilder instance in\u001b[39;00m\n\u001b[1;32m    256\u001b[0m \u001b[39m# builder, or we have a builder_class that we can instantiate\u001b[39;00m\n\u001b[1;32m    257\u001b[0m \u001b[39m# with the remaining **kwargs.\u001b[39;00m\n\u001b[1;32m    258\u001b[0m \u001b[39mif\u001b[39;00m builder \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mFeatureNotFound\u001b[0m: Couldn't find a tree builder with the features you requested: lxml. Do you need to install a parser library?"
     ]
    }
   ],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restarted Python 3.9.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2b4bab-2370-4842-ad63-500cdb312731",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sarahkim/Library/Python/3.9/lib/python/site-packages/urllib3/__init__.py:35: NotOpenSSLWarning: urllib3 v2 only supports OpenSSL 1.1.1+, currently the 'ssl' module is compiled with 'LibreSSL 2.8.3'. See: https://github.com/urllib3/urllib3/issues/3020\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "import re\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.renderers.enable(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b15aedfe-7a80-4de6-a67c-9175f56880f9",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [],
   "source": [
    "url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1726c405-c0c3-41fa-8af5-7b512a842440",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enforcement_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     category_tag \u001b[39m=\u001b[39m li_soup\u001b[39m.\u001b[39mselect_one(\u001b[39m'\u001b[39m\u001b[39mul li\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     category \u001b[39m=\u001b[39m category_tag\u001b[39m.\u001b[39mget_text(strip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m     enforcement_actions\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     19\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m: title,\n\u001b[1;32m     20\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mLink\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov\u001b[39m\u001b[39m{\u001b[39;00mlink\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m: date,\n\u001b[1;32m     22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m'\u001b[39m: category\n\u001b[1;32m     23\u001b[0m     })\n\u001b[1;32m     25\u001b[0m enforcement_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(enforcement_actions)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(enforcement_df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enforcement_actions' is not defined"
     ]
    }
   ],
   "source": [
    "main_tag = soup.find('main')\n",
    "li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "li_with_div_content = [li.contents for li in li_with_div]\n",
    "\n",
    "for li in li_with_div_content[:19]:  \n",
    "    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')\n",
    "    \n",
    "    title_tag = li_soup.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True) \n",
    "    link = title_tag['href'] \n",
    "    \n",
    "    date_tag = li_soup.select_one('span')\n",
    "    date = date_tag.get_text(strip=True) \n",
    "    \n",
    "    category_tag = li_soup.select_one('ul li')\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    \n",
    "    enforcement_actions.append({\n",
    "        'Title': title,\n",
    "        'Link': f\"https://oig.hhs.gov{link}\",\n",
    "        'Date': date,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70ad712d-b8fa-4708-839d-1d0c92c70151",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enforcement_actions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:18\u001b[0m\n\u001b[1;32m     15\u001b[0m     category_tag \u001b[39m=\u001b[39m li_soup\u001b[39m.\u001b[39mselect_one(\u001b[39m'\u001b[39m\u001b[39mul li\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     16\u001b[0m     category \u001b[39m=\u001b[39m category_tag\u001b[39m.\u001b[39mget_text(strip\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m---> 18\u001b[0m     enforcement_actions\u001b[39m.\u001b[39mappend({\n\u001b[1;32m     19\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mTitle\u001b[39m\u001b[39m'\u001b[39m: title,\n\u001b[1;32m     20\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mLink\u001b[39m\u001b[39m'\u001b[39m: \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov\u001b[39m\u001b[39m{\u001b[39;00mlink\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     21\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mDate\u001b[39m\u001b[39m'\u001b[39m: date,\n\u001b[1;32m     22\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mCategory\u001b[39m\u001b[39m'\u001b[39m: category\n\u001b[1;32m     23\u001b[0m     })\n\u001b[1;32m     25\u001b[0m enforcement_df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(enforcement_actions)\n\u001b[1;32m     26\u001b[0m \u001b[39mprint\u001b[39m(enforcement_df\u001b[39m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enforcement_actions' is not defined"
     ]
    }
   ],
   "source": [
    "main_tag = soup.find('main')\n",
    "li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "li_with_div_content = [li.contents for li in li_with_div]\n",
    "\n",
    "for li in li_with_div_content[:19]:  \n",
    "    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')\n",
    "    \n",
    "    title_tag = li_soup.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True) \n",
    "    link = title_tag['href'] \n",
    "    \n",
    "    date_tag = li_soup.select_one('span')\n",
    "    date = date_tag.get_text(strip=True) \n",
    "    \n",
    "    category_tag = li_soup.select_one('ul li')\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    \n",
    "    enforcement_actions.append({\n",
    "        'Title': title,\n",
    "        'Link': f\"https://oig.hhs.gov{link}\",\n",
    "        'Date': date,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85ec52bd-4a75-4ef1-a520-212ce41f8714",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Attorney General Griffin Announces Medicaid Fr...   \n",
      "\n",
      "                                                Link              Date  \\\n",
      "0  https://oig.hhs.gov/fraud/enforcement/attorney...  October 25, 2024   \n",
      "\n",
      "                     Category  \n",
      "0  State Enforcement Agencies  \n"
     ]
    }
   ],
   "source": [
    "main_tag = soup.find('main')\n",
    "li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "li_with_div_content = [li.contents for li in li_with_div]\n",
    "\n",
    "for li in li_with_div_content[:19]:  \n",
    "    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')\n",
    "    \n",
    "    title_tag = li_soup.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True) \n",
    "    link = title_tag['href'] \n",
    "    \n",
    "    date_tag = li_soup.select_one('span')\n",
    "    date = date_tag.get_text(strip=True) \n",
    "    \n",
    "    category_tag = li_soup.select_one('ul li')\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    \n",
    "    enforcement_actions = []\n",
    "    enforcement_actions.append({\n",
    "        'Title': title,\n",
    "        'Link': f\"https://oig.hhs.gov{link}\",\n",
    "        'Date': date,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e7432ed-96ba-440b-80a0-7dc955bf5e7a",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Macomb County Doctor And Pharmacist Agree To P...   \n",
      "1  Rocky Hill Pharmacy And Its Owners Indicted Fo...   \n",
      "2  North Texas Medical Center Pays $14.2 Million ...   \n",
      "3  New England Doctor Pleads Guilty To Drug Distr...   \n",
      "4  St. Louis County Woman Accused Of $3 Million H...   \n",
      "\n",
      "                                                Link              Date  \\\n",
      "0  https://oig.hhs.gov/fraud/enforcement/macomb-c...  November 4, 2024   \n",
      "1  https://oig.hhs.gov/fraud/enforcement/rocky-hi...  November 4, 2024   \n",
      "2  https://oig.hhs.gov/fraud/enforcement/north-te...  November 4, 2024   \n",
      "3  https://oig.hhs.gov/fraud/enforcement/new-engl...  November 4, 2024   \n",
      "4  https://oig.hhs.gov/fraud/enforcement/st-louis...  November 1, 2024   \n",
      "\n",
      "                     Category  \n",
      "0  Criminal and Civil Actions  \n",
      "1  Criminal and Civil Actions  \n",
      "2  Criminal and Civil Actions  \n",
      "3  Criminal and Civil Actions  \n",
      "4  Criminal and Civil Actions  \n"
     ]
    }
   ],
   "source": [
    "enforcement_actions = []\n",
    "main_tag = soup.find('main')\n",
    "li_with_div = soup.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "li_with_div_content = [li.contents for li in li_with_div]\n",
    "\n",
    "for li in li_with_div_content[:19]:  \n",
    "    li_soup = BeautifulSoup(''.join(str(item) for item in li), 'html.parser')\n",
    "    \n",
    "    title_tag = li_soup.select_one('h2 a')\n",
    "    title = title_tag.get_text(strip=True) \n",
    "    link = title_tag['href'] \n",
    "    \n",
    "    date_tag = li_soup.select_one('span')\n",
    "    date = date_tag.get_text(strip=True) \n",
    "    \n",
    "    category_tag = li_soup.select_one('ul li')\n",
    "    category = category_tag.get_text(strip=True)\n",
    "    \n",
    "    enforcement_actions.append({\n",
    "        'Title': title,\n",
    "        'Link': f\"https://oig.hhs.gov{link}\",\n",
    "        'Date': date,\n",
    "        'Category': category\n",
    "    })\n",
    "\n",
    "enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b2a58a03-eed0-4eba-ac0d-df76908c4006",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. Attorney's Office, Northern District of Texas\n"
     ]
    }
   ],
   "source": [
    "url_1 = \"https://oig.hhs.gov/fraud/enforcement/north-texas-medical-center-pays-142-million-to-resolve-potential-false-claims-act-liability-for-self-reported-violations-of-medicare-regs-stark-law/\"\n",
    "\n",
    "response_1 = requests.get(url_1)\n",
    "soup_1= BeautifulSoup(response_1.content, 'html.parser')\n",
    "\n",
    "li_tags = soup_1.find_all('li')\n",
    "for li in li_tags:\n",
    "    agency_span = li.find('span')\n",
    "    \n",
    "    if agency_span and \"Agency\" in agency_span.text:  \n",
    "        agency_name = agency_span.find_next_sibling(text=True)\n",
    "        \n",
    "        if agency_name:\n",
    "            print(agency_name.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34f94b98-56b4-4eb8-94f6-72cb16d4bc62",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               Title  \\\n",
      "0  Macomb County Doctor And Pharmacist Agree To P...   \n",
      "1  Rocky Hill Pharmacy And Its Owners Indicted Fo...   \n",
      "2  North Texas Medical Center Pays $14.2 Million ...   \n",
      "3  New England Doctor Pleads Guilty To Drug Distr...   \n",
      "4  St. Louis County Woman Accused Of $3 Million H...   \n",
      "\n",
      "                                                Link              Date  \\\n",
      "0  https://oig.hhs.gov/fraud/enforcement/macomb-c...  November 4, 2024   \n",
      "1  https://oig.hhs.gov/fraud/enforcement/rocky-hi...  November 4, 2024   \n",
      "2  https://oig.hhs.gov/fraud/enforcement/north-te...  November 4, 2024   \n",
      "3  https://oig.hhs.gov/fraud/enforcement/new-engl...  November 4, 2024   \n",
      "4  https://oig.hhs.gov/fraud/enforcement/st-louis...  November 1, 2024   \n",
      "\n",
      "                     Category  \\\n",
      "0  Criminal and Civil Actions   \n",
      "1  Criminal and Civil Actions   \n",
      "2  Criminal and Civil Actions   \n",
      "3  Criminal and Civil Actions   \n",
      "4  Criminal and Civil Actions   \n",
      "\n",
      "                                              Agency  \n",
      "0  U.S. Attorney's Office, Eastern District of Mi...  \n",
      "1  U.S. Attorney's Office, Eastern District of Te...  \n",
      "2  U.S. Attorney's Office, Northern District of T...  \n",
      "3                         U.S. Department of Justice  \n",
      "4  U.S. Attorney's Office, Eastern District of Mi...  \n"
     ]
    }
   ],
   "source": [
    "agency_names = []\n",
    "\n",
    "for link in enforcement_df['Link']:\n",
    "    response_1 = requests.get(link)\n",
    "    soup_1 = BeautifulSoup(response_1.content, 'html.parser')\n",
    "    \n",
    "    li_tags = soup_1.find_all('li')\n",
    "    agency_name = None\n",
    "    \n",
    "    for li in li_tags:\n",
    "        agency_span = li.find('span')\n",
    "        \n",
    "        if agency_span and \"Agency\" in agency_span.text: \n",
    "            agency_name = agency_span.find_next_sibling(text=True)\n",
    "            if agency_name:\n",
    "                agency_name = agency_name.strip()  \n",
    "\n",
    "    agency_names.append(agency_name)\n",
    "\n",
    "enforcement_df['Agency'] = agency_names\n",
    "print(enforcement_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b88a513-062d-4575-b77d-dba45ec0e6cd",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title       St. Louis County Woman Accused Of $3 Million H...\n",
      "Link        https://oig.hhs.gov/fraud/enforcement/st-louis...\n",
      "Date                                         November 1, 2024\n",
      "Category                           Criminal and Civil Actions\n",
      "Agency      U.S. Attorney's Office, Eastern District of Mi...\n",
      "Name: 4, dtype: object\n"
     ]
    }
   ],
   "source": [
    "earliest_action = enforcement_df.sort_values(by='Date').iloc[0]\n",
    "print(earliest_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e76083ff-56e8-42a3-b812-514a1d5e890b",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:64\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[39mreturn\u001b[39;00m enforcement_df\n\u001b[1;32m     63\u001b[0m \u001b[39m# Run the function to collect data from January 2023 onward\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m enforcement_df \u001b[39m=\u001b[39m scrape_enforcement_actions(\u001b[39m1\u001b[39;49m, \u001b[39m2023\u001b[39;49m)\n",
      "File \u001b[1;32m/Users/sarahkim/Documents/Coding/problem-set-5-maryell-sarah/ps5_template.qmd:11\u001b[0m\n\u001b[1;32m      9\u001b[0m base_url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://oig.hhs.gov/fraud/enforcement/\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     10\u001b[0m page_url \u001b[39m=\u001b[39m base_url\n\u001b[0;32m---> 11\u001b[0m start_date \u001b[39m=\u001b[39m datetime(year, month, \u001b[39m1\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     14\u001b[0m     \u001b[39m# Fetch the page content\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     response \u001b[39m=\u001b[39m requests\u001b[39m.\u001b[39mget(page_url)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "def scrape_enforcement_actions(month, year):\n",
    "    # Validate the year input\n",
    "    if year < 2013:\n",
    "        print(\"Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty list to store enforcement actions\n",
    "    enforcement_actions = []\n",
    "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "    page_url = base_url\n",
    "    start_date = datetime(year, month, 1)\n",
    "\n",
    "    while True:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find enforcement actions on the page\n",
    "        main_tag = soup.find('main')\n",
    "        li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "        \n",
    "        for li in li_with_div:\n",
    "            li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')\n",
    "            \n",
    "            title_tag = li_soup.select_one('h2 a')\n",
    "            date_tag = li_soup.select_one('span')\n",
    "            category_tag = li_soup.select_one('ul li')\n",
    "            \n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "            link = title_tag['href'] if title_tag else None\n",
    "            date_str = date_tag.get_text(strip=True) if date_tag else None\n",
    "            category = category_tag.get_text(strip=True) if category_tag else None\n",
    "            \n",
    "            # Format the date and check if it’s within the specified range\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if date < start_date:\n",
    "                return pd.DataFrame(enforcement_actions)  # Return DataFrame when date is out of range\n",
    "\n",
    "            # Add to the list\n",
    "            enforcement_actions.append({\n",
    "                'Title': title,\n",
    "                'Link': f\"https://oig.hhs.gov{link}\",\n",
    "                'Date': date,\n",
    "                'Category': category\n",
    "            })\n",
    "        \n",
    "        # Move to the next page if available\n",
    "        next_page = soup.select_one(\"a[rel='next']\")\n",
    "        if next_page:\n",
    "            page_url = base_url + next_page['href']\n",
    "            sleep(1)  # Wait 1 second to avoid server overloading\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "    return enforcement_df\n",
    "\n",
    "# Run the function to collect data from January 2023 onward\n",
    "enforcement_df = scrape_enforcement_actions(1, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65cc8dcf-1eb3-496f-a6ba-3727269c4148",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RendererRegistry.enable('png')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import altair as alt\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import warnings \n",
    "import re\n",
    "from time import sleep\n",
    "from datetime import datetime\n",
    "warnings.filterwarnings('ignore')\n",
    "alt.renderers.enable(\"png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50e6e3fa-3563-4bb3-93f9-c8148c9a6d55",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_enforcement_actions(month, year):\n",
    "    # Validate the year input\n",
    "    if year < 2013:\n",
    "        print(\"Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty list to store enforcement actions\n",
    "    enforcement_actions = []\n",
    "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "    page_url = base_url\n",
    "    start_date = datetime(year, month, 1)\n",
    "\n",
    "    while True:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find enforcement actions on the page\n",
    "        main_tag = soup.find('main')\n",
    "        li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "        \n",
    "        for li in li_with_div:\n",
    "            li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')\n",
    "            \n",
    "            title_tag = li_soup.select_one('h2 a')\n",
    "            date_tag = li_soup.select_one('span')\n",
    "            category_tag = li_soup.select_one('ul li')\n",
    "            \n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "            link = title_tag['href'] if title_tag else None\n",
    "            date_str = date_tag.get_text(strip=True) if date_tag else None\n",
    "            category = category_tag.get_text(strip=True) if category_tag else None\n",
    "            \n",
    "            # Format the date and check if it’s within the specified range\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if date < start_date:\n",
    "                return pd.DataFrame(enforcement_actions)  # Return DataFrame when date is out of range\n",
    "\n",
    "            # Add to the list\n",
    "            enforcement_actions.append({\n",
    "                'Title': title,\n",
    "                'Link': f\"https://oig.hhs.gov{link}\",\n",
    "                'Date': date,\n",
    "                'Category': category\n",
    "            })\n",
    "        \n",
    "        # Move to the next page if available\n",
    "        next_page = soup.select_one(\"a[rel='next']\")\n",
    "        if next_page:\n",
    "            page_url = base_url + next_page['href']\n",
    "            sleep(1)  # Wait 1 second to avoid server overloading\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "    return enforcement_df\n",
    "\n",
    "# Run the function to collect data from January 2023 onward\n",
    "enforcement_df = scrape_enforcement_actions(1, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c08a5de0-9662-40c9-8321-a0e4a05ed23a",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total enforcement actions since January 2023: 20\n"
     ]
    }
   ],
   "source": [
    "total_actions = len(enforcement_df)\n",
    "print(\"Total enforcement actions since January 2023:\", total_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "64282405-4b2e-4a01-8b81-aea3f79bd3da",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total enforcement actions since January 2023: 20\n",
      "Earliest enforcement action:\n",
      " Title       Arizona Doctor Sentenced To Prison For Health ...\n",
      "Link        https://oig.hhs.gov/fraud/enforcement/arizona-...\n",
      "Date                                      2024-10-24 00:00:00\n",
      "Category                           Criminal and Civil Actions\n",
      "Name: 19, dtype: object\n"
     ]
    }
   ],
   "source": [
    "total_actions = len(enforcement_df)\n",
    "print(\"Total enforcement actions since January 2023:\", total_actions)\n",
    "\n",
    "earliest_action = enforcement_df.sort_values(by='Date').iloc[0]\n",
    "print(\"Earliest enforcement action:\\n\", earliest_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4dda3296-9d4d-450b-98d3-8d0f9ece1f41",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [],
   "source": [
    "def scrape_enforcement_actions(month, year):\n",
    "    # Validate the year input\n",
    "    if year < 2013:\n",
    "        print(\"Only enforcement actions from 2013 onwards are available. Please enter a year >= 2013.\")\n",
    "        return None\n",
    "    \n",
    "    # Initialize an empty list to store enforcement actions\n",
    "    enforcement_actions = []\n",
    "    base_url = \"https://oig.hhs.gov/fraud/enforcement/\"\n",
    "    page_url = base_url\n",
    "    start_date = datetime(year, month, 1)\n",
    "\n",
    "    while True:\n",
    "        # Fetch the page content\n",
    "        response = requests.get(page_url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Find enforcement actions on the page\n",
    "        main_tag = soup.find('main')\n",
    "        li_with_div = main_tag.find_all(lambda t: t.name == 'li' and t.find('div'))\n",
    "        \n",
    "        for li in li_with_div:\n",
    "            li_soup = BeautifulSoup(''.join(str(item) for item in li.contents), 'html.parser')\n",
    "            \n",
    "            title_tag = li_soup.select_one('h2 a')\n",
    "            date_tag = li_soup.select_one('span')\n",
    "            category_tag = li_soup.select_one('ul li')\n",
    "            \n",
    "            title = title_tag.get_text(strip=True) if title_tag else None\n",
    "            link = title_tag['href'] if title_tag else None\n",
    "            date_str = date_tag.get_text(strip=True) if date_tag else None\n",
    "            category = category_tag.get_text(strip=True) if category_tag else None\n",
    "            \n",
    "            # Format the date and check if it’s within the specified range\n",
    "            try:\n",
    "                date = datetime.strptime(date_str, \"%B %d, %Y\")\n",
    "            except ValueError:\n",
    "                continue\n",
    "            \n",
    "            if date < start_date:\n",
    "                return pd.DataFrame(enforcement_actions)  # Return DataFrame when date is out of range\n",
    "\n",
    "            # Add to the list\n",
    "            enforcement_actions.append({\n",
    "                'Title': title,\n",
    "                'Link': f\"https://oig.hhs.gov{link}\",\n",
    "                'Date': date,\n",
    "                'Category': category\n",
    "            })\n",
    "        \n",
    "        # Move to the next page if available\n",
    "        next_page = soup.select_one(\"a[rel='next']\")\n",
    "        if next_page:\n",
    "            page_url = base_url + next_page['href']\n",
    "            sleep(1)  # Wait 1 second to avoid server overloading\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    enforcement_df = pd.DataFrame(enforcement_actions)\n",
    "    return enforcement_df\n",
    "\n",
    "# Run the function to collect data from January 2023 onward\n",
    "enforcement_df = scrape_enforcement_actions(1, 2023)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51f3e229-e303-423b-9603-543f0be5ae90",
   "metadata": {
    "vscode": {
     "languageId": "quarto"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total enforcement actions since January 2023: 20\n",
      "Earliest enforcement action:\n",
      " Title       Arizona Doctor Sentenced To Prison For Health ...\n",
      "Link        https://oig.hhs.gov/fraud/enforcement/arizona-...\n",
      "Date                                      2024-10-24 00:00:00\n",
      "Category                           Criminal and Civil Actions\n",
      "Name: 19, dtype: object\n"
     ]
    }
   ],
   "source": [
    "total_actions = len(enforcement_df)\n",
    "print(\"Total enforcement actions since January 2023:\", total_actions)\n",
    "\n",
    "earliest_action = enforcement_df.sort_values(by='Date').iloc[0]\n",
    "print(\"Earliest enforcement action:\\n\", earliest_action)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
